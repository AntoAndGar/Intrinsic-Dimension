{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")\n",
    "\n",
    "# add reproducibility stuff\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastfoodWrapper(nn.Module):\n",
    "    def __init__(self, module, intrinsic_dimension, device):\n",
    "        \"\"\"\n",
    "        Wrapper to estimate the intrinsic dimensionality of the\n",
    "        objective landscape for a specific task given a specific model using FastFood transform\n",
    "        :param module: pytorch nn.Module\n",
    "        :param intrinsic_dimension: dimensionality within which we search for solution\n",
    "        :param device: cuda device id\n",
    "        \"\"\"\n",
    "        super(FastfoodWrapper, self).__init__()\n",
    "\n",
    "        # Hide this from inspection by get_parameters()\n",
    "        self.m = [module]\n",
    "\n",
    "        self.name_base_localname = []\n",
    "\n",
    "        # Stores the initial value: \\theta_{0}^{D}\n",
    "        self.initial_value = dict()\n",
    "\n",
    "        # Fastfood parameters\n",
    "        self.fastfood_params = {}\n",
    "\n",
    "        # Parameter vector that is updated\n",
    "        # Initialised with zeros as per text: \\theta^{d}\n",
    "        V = nn.Parameter(torch.zeros((intrinsic_dimension)).to(device))\n",
    "        self.register_parameter(\"V\", V)\n",
    "        v_size = (intrinsic_dimension,)\n",
    "\n",
    "        # Iterate over layers in the module\n",
    "        for name, param in module.named_parameters():\n",
    "            # If param requires grad update\n",
    "            if param.requires_grad:\n",
    "\n",
    "                # Saves the initial values of the initialised parameters from param.data and sets them to no grad.\n",
    "                # (initial values are the 'origin' of the search)\n",
    "                self.initial_value[name] = v0 = (\n",
    "                    param.clone().detach().requires_grad_(False).to(device)\n",
    "                )\n",
    "\n",
    "                # Generate fastfood parameters\n",
    "                DD = np.prod(v0.size())\n",
    "                self.fastfood_params[name] = fastfood_vars(DD, device)\n",
    "\n",
    "                base, localname = module, name\n",
    "                while \".\" in localname:\n",
    "                    prefix, localname = localname.split(\".\", 1)\n",
    "                    base = base.__getattr__(prefix)\n",
    "                self.name_base_localname.append((name, base, localname))\n",
    "\n",
    "        for name, base, localname in self.name_base_localname:\n",
    "            delattr(base, localname)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Iterate over layers\n",
    "        for name, base, localname in self.name_base_localname:\n",
    "\n",
    "            init_shape = self.initial_value[name].size()\n",
    "            DD = np.prod(init_shape)\n",
    "\n",
    "            # Fastfood transform te replace dence P\n",
    "            ray = fastfood_torched(self.V, DD, self.fastfood_params[name]).view(\n",
    "                init_shape\n",
    "            )\n",
    "\n",
    "            param = self.initial_value[name] + ray\n",
    "\n",
    "            setattr(base, localname, param)\n",
    "\n",
    "        # Pass through the model, by getting hte module from a list self.m\n",
    "        module = self.m[0]\n",
    "        x = module(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_walsh_hadamard_torched(x, axis=0, normalize=False):\n",
    "    \"\"\"\n",
    "    Performs fast Walsh Hadamard transform\n",
    "    :param x:\n",
    "    :param axis:\n",
    "    :param normalize:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    orig_shape = x.size()\n",
    "    assert axis >= 0 and axis < len(\n",
    "        orig_shape\n",
    "    ), \"For a vector of shape %s, axis must be in [0, %d] but it is %d\" % (\n",
    "        orig_shape,\n",
    "        len(orig_shape) - 1,\n",
    "        axis,\n",
    "    )\n",
    "    h_dim = orig_shape[axis]\n",
    "    h_dim_exp = int(round(np.log(h_dim) / np.log(2)))\n",
    "    assert h_dim == 2**h_dim_exp, (\n",
    "        \"hadamard can only be computed over axis with size that is a power of two, but\"\n",
    "        \" chosen axis %d has size %d\" % (axis, h_dim)\n",
    "    )\n",
    "\n",
    "    working_shape_pre = [int(np.prod(orig_shape[:axis]))]  # prod of empty array is 1 :)\n",
    "    working_shape_post = [\n",
    "        int(np.prod(orig_shape[axis + 1 :]))\n",
    "    ]  # prod of empty array is 1 :)\n",
    "    working_shape_mid = [2] * h_dim_exp\n",
    "    working_shape = working_shape_pre + working_shape_mid + working_shape_post\n",
    "\n",
    "    ret = x.view(working_shape)\n",
    "\n",
    "    for ii in range(h_dim_exp):\n",
    "        dim = ii + 1\n",
    "        arrs = torch.chunk(ret, 2, dim=dim)\n",
    "        assert len(arrs) == 2\n",
    "        ret = torch.cat((arrs[0] + arrs[1], arrs[0] - arrs[1]), axis=dim)\n",
    "\n",
    "    if normalize:\n",
    "        ret = ret / torch.sqrt(float(h_dim))\n",
    "\n",
    "    ret = ret.view(orig_shape)\n",
    "\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastfood_vars(DD, device=0):\n",
    "    \"\"\"\n",
    "    Returns parameters for fast food transform\n",
    "    :param DD: desired dimension\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ll = int(np.ceil(np.log(DD) / np.log(2)))\n",
    "    LL = 2**ll\n",
    "\n",
    "    # Binary scaling matrix where $B_{i,i} \\in \\{\\pm 1 \\}$ drawn iid\n",
    "    BB = torch.FloatTensor(LL).uniform_(0, 2).type(torch.LongTensor)\n",
    "    BB = (BB * 2 - 1).type(torch.FloatTensor).to(device)\n",
    "    BB.requires_grad = False\n",
    "\n",
    "    # Random permutation matrix\n",
    "    Pi = torch.LongTensor(np.random.permutation(LL)).to(device)\n",
    "    Pi.requires_grad = False\n",
    "\n",
    "    # Gaussian scaling matrix, whose elements $G_{i,i} \\sim \\mathcal{N}(0, 1)$\n",
    "    GG = (\n",
    "        torch.FloatTensor(\n",
    "            LL,\n",
    "        )\n",
    "        .normal_()\n",
    "        .to(device)\n",
    "    )\n",
    "    GG.requires_grad = False\n",
    "\n",
    "    divisor = torch.sqrt(LL * torch.sum(torch.pow(GG, 2)))\n",
    "\n",
    "    return [BB, Pi, GG, divisor, LL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastfood_torched(x, DD, param_list=None, device=0):\n",
    "    \"\"\"\n",
    "    Fastfood transform\n",
    "    :param x: array of dd dimension\n",
    "    :param DD: desired dimension\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dd = x.size(0)\n",
    "\n",
    "    if not param_list:\n",
    "\n",
    "        BB, Pi, GG, divisor, LL = fastfood_vars(DD, device=device)\n",
    "\n",
    "    else:\n",
    "\n",
    "        BB, Pi, GG, divisor, LL = param_list\n",
    "\n",
    "    # Padd x if needed\n",
    "    dd_pad = F.pad(x, pad=(0, LL - dd), value=0, mode=\"constant\")\n",
    "\n",
    "    # From left to right HGPiH(BX), where H is Walsh-Hadamard matrix\n",
    "    mul_1 = torch.mul(BB, dd_pad)\n",
    "    # HGPi(HBX)\n",
    "    mul_2 = fast_walsh_hadamard_torched(mul_1, 0, normalize=False)\n",
    "\n",
    "    # HG(PiHBX)\n",
    "    mul_3 = mul_2[Pi]\n",
    "\n",
    "    # H(GPiHBX)\n",
    "    mul_4 = torch.mul(mul_3, GG)\n",
    "\n",
    "    # (HGPiHBX)\n",
    "    mul_5 = fast_walsh_hadamard_torched(mul_4, 0, normalize=False)\n",
    "\n",
    "    ret = torch.div(mul_5[:DD], divisor * np.sqrt(float(DD) / LL))\n",
    "\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseWrap(nn.Module):\n",
    "    def __init__(self, module, intrinsic_dimension, device):\n",
    "        \"\"\"\n",
    "        Wrapper to estimate the intrinsic dimensionality of the\n",
    "        objective landscape for a specific task given a specific model\n",
    "        :param module: pytorch nn.Module\n",
    "        :param intrinsic_dimension: dimensionality within which we search for solution\n",
    "        :param device: cuda device id\n",
    "        \"\"\"\n",
    "        super(DenseWrap, self).__init__()\n",
    "\n",
    "        # Hide this from inspection by get_parameters()\n",
    "        self.m = [module]\n",
    "\n",
    "        self.name_base_localname = []\n",
    "\n",
    "        # Stores the initial value: \\theta_{0}^{D}\n",
    "        self.initial_value = dict()\n",
    "\n",
    "        # Stores the randomly generated projection matrix P\n",
    "        self.random_matrix = dict()\n",
    "\n",
    "        # Parameter vector that is updated, initialised with zeros as per text: \\theta^{d}\n",
    "        V = nn.Parameter(torch.zeros((intrinsic_dimension, 1)).to(device))\n",
    "        self.register_parameter(\"V\", V)\n",
    "        v_size = (intrinsic_dimension,)\n",
    "\n",
    "        # Iterates over layers in the Neural Network\n",
    "        for name, param in module.named_parameters():\n",
    "            # If the parameter requires gradient update\n",
    "            if param.requires_grad:\n",
    "\n",
    "                # Saves the initial values of the initialised parameters from param.data and sets them to no grad.\n",
    "                # (initial values are the 'origin' of the search)\n",
    "                self.initial_value[name] = v0 = (\n",
    "                    param.clone().detach().requires_grad_(False).to(device)\n",
    "                )\n",
    "\n",
    "                # If v0.size() is [4, 3], then below operation makes it [4, 3, v_size]\n",
    "                matrix_size = v0.size() + v_size\n",
    "\n",
    "                # Generates random projection matrices P, sets them to no grad\n",
    "                self.random_matrix[name] = (\n",
    "                    torch.randn(matrix_size, requires_grad=False).to(device)\n",
    "                    / intrinsic_dimension**0.5\n",
    "                )\n",
    "\n",
    "                # NOTE!: lines below are not clear!\n",
    "                base, localname = module, name\n",
    "                while \".\" in localname:\n",
    "                    prefix, localname = localname.split(\".\", 1)\n",
    "                    base = base.__getattr__(prefix)\n",
    "                self.name_base_localname.append((name, base, localname))\n",
    "\n",
    "        for name, base, localname in self.name_base_localname:\n",
    "            delattr(base, localname)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Iterate over the layers\n",
    "        for name, base, localname in self.name_base_localname:\n",
    "\n",
    "            # Product between matrix P and \\theta^{d}\n",
    "            ray = torch.matmul(self.random_matrix[name], self.V)\n",
    "\n",
    "            # Add the \\theta_{0}^{D} to P \\dot \\theta^{d}\n",
    "            param = self.initial_value[name] + torch.squeeze(ray, -1)\n",
    "\n",
    "            setattr(base, localname, param)\n",
    "\n",
    "        # Pass through the model, by getting the module from a list self.m\n",
    "        module = self.m[0]\n",
    "        x = module(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YOUR_NUMBER_OF_CLASSES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ant/All/Sapienza/Deep Learning & AAI/Intrinsic-Dimension/intrinsic.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Get model and wrap it in fastfood\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m model \u001b[39m=\u001b[39m get_resnet(\u001b[39m\"\u001b[39m\u001b[39mresnet18\u001b[39m\u001b[39m\"\u001b[39m, num_classes\u001b[39m=\u001b[39mYOUR_NUMBER_OF_CLASSES)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m model \u001b[39m=\u001b[39m FastfoodWrapper(model, intrinsic_dimension\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YOUR_NUMBER_OF_CLASSES' is not defined"
     ]
    }
   ],
   "source": [
    "# implementig the code from the paper https://arxiv.org/abs/1804.08838 in pytorch\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, n_classes)\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_resnet(encoder_name, num_classes, pretrained=False):\n",
    "    assert encoder_name in [\n",
    "        \"resnet18\",\n",
    "        \"resnet50\",\n",
    "    ], \"{} is a wrong encoder name!\".format(encoder_name)\n",
    "    if encoder_name == \"resnet18\":\n",
    "        model = models.resnet18(pretrained=pretrained)\n",
    "        latent_dim = 512\n",
    "    else:\n",
    "        model = models.resnet50(pretrained=pretrained)\n",
    "        latent_dim = 2048\n",
    "    children = list(model.children())[:-2] + [Classifier(latent_dim, num_classes)]\n",
    "    model = torch.nn.Sequential(*children)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get model and wrap it in fastfood\n",
    "model = get_resnet(\"resnet18\", num_classes=YOUR_NUMBER_OF_CLASSES).cuda()\n",
    "model = FastfoodWrapper(model, intrinsic_dimension=100, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for a Fully Connected Network\n",
    "class FullyConnectedNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Standard LeNet Network, reference http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf with some modification to follow the same number of parameters as the main paper for the task does\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 6 kernels 5x5\n",
    "        self.conv1 = nn.Conv2d(input_dim, 6, 5, padding='valid',)\n",
    "        # max-pooling over 2x2\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        # 16 kernels 5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding='valid')\n",
    "        # max-pooling over 2x2\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        # 120 kernels 4x4 to match the dimensionality of the fully connected network\n",
    "        self.conv3 = nn.Conv2d(16, 120, 4,)\n",
    "        # 120 fully connected neurons, too many parameter in this case w.r.t. the paper\n",
    "        #self.fc1 = nn.Linear(16 * 5 * 5, 120,)\n",
    "        self.flat = nn.Flatten(start_dim=1)\n",
    "        # 84 fully connected neurons\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # 10 fully connected neurons\n",
    "        self.fc3 = nn.Linear(84, output_dim,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flat(x)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "# from https://discuss.pytorch.org/t/locally-connected-layers/26979\n",
    "class LocallyConnected2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, output_size, kernel_size, stride=1, bias=True):\n",
    "        super(LocallyConnected2d, self).__init__()\n",
    "        output_size = _pair(output_size)\n",
    "        self.weight = nn.Parameter(\n",
    "            nn.init.kaiming_normal_(torch.randn(1, out_channels, in_channels, output_size[0], output_size[1], kernel_size**2), nonlinearity='relu')\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(nn.init.kaiming_normal_(\n",
    "                torch.randn(1, out_channels, output_size[0], output_size[1]), nonlinearity='relu')\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = _pair(stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, c, h, w = x.size()\n",
    "        kh, kw = self.kernel_size\n",
    "        dh, dw = self.stride\n",
    "        x = x.unfold(2, kh, dh).unfold(3, kw, dw)\n",
    "        x = x.contiguous().view(*x.size()[:-2], -1)\n",
    "        # Sum in in_channel and kernel_size dims\n",
    "        out = (x.unsqueeze(1) * self.weight).sum([2, -1])\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        return out\n",
    "\n",
    "# Class for Untied LeNet Network\n",
    "class Untied_LeNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Untied_LeNet, self).__init__()\n",
    "        # 6 kernels 5x5\n",
    "        self.conv1 = LocallyConnected2d(input_dim, 6, (24,24), 5)\n",
    "        # max-pooling over 2x2\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        # 16 kernels 5x5\n",
    "        self.conv2 = LocallyConnected2d(6, 16, (8,8), 5)\n",
    "        # max-pooling over 2x2\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        # 120 kernels 4x4 to match the dimensionality of the fully connected network\n",
    "        self.conv3 = LocallyConnected2d(16, 120, (1,1), 4)\n",
    "        # 120 fully connected neurons, too many parameter in this case w.r.t. the paper\n",
    "        #self.fc1 = nn.Linear(16 * 5 * 5, 120,)\n",
    "        self.flat = nn.Flatten(start_dim=1)\n",
    "        # 84 fully connected neurons\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # 10 fully connected neurons\n",
    "        self.fc3 = nn.Linear(84, output_dim,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flat(x)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for FC-LeNet Network\n",
    "class FcLeNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FcLeNet, self).__init__()\n",
    "        # 6 kernels 5x5\n",
    "        self.fcconv1 = nn.Linear(input_dim, 3456)\n",
    "        # max-pooling over 2x2\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        # 16 kernels 5x5\n",
    "        self.fcconv2 = nn.Linear(864, 1024)\n",
    "        # max-pooling over 2x2\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        # 120 kernels 4x4 to match the dimensionality of the fully connected network\n",
    "        self.fcconv3 = nn.Linear(256, 120)\n",
    "        # 120 fully connected neurons, too many parameter in this case w.r.t. the paper\n",
    "        #self.fc1 = nn.Linear(16 * 5 * 5, 120,)\n",
    "        self.flat = nn.Flatten(start_dim=1)\n",
    "        # 84 fully connected neurons\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # 10 fully connected neurons\n",
    "        self.fc3 = nn.Linear(84, output_dim,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.fcconv1(x)).view(-1,6,24,24))\n",
    "        x = self.flat(x)\n",
    "        x = self.pool2(F.relu(self.fcconv2(x)).view(-1,16,8,8))\n",
    "        x = self.flat(x)\n",
    "        x = F.relu(self.fcconv3(x))\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for FCTied-LeNet\n",
    "class FCTied_LeNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FCTied_LeNet, self).__init__()\n",
    "        # 6 kernels 5x5\n",
    "        self.conv1 = nn.Conv2d(input_dim, 6, 55, padding='same',)\n",
    "        # max-pooling over 2x2\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "        # 16 kernels 5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 27, padding='same')\n",
    "        # max-pooling over 2x2\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        # 120 kernels 4x4 to match the dimensionality of the fully connected network\n",
    "        self.conv3 = nn.Conv2d(16, 120, 7,)\n",
    "        # 120 fully connected neurons, too many parameter in this case w.r.t. the paper\n",
    "        #self.fc1 = nn.Linear(16 * 5 * 5, 120,)\n",
    "        self.flat = nn.Flatten(start_dim=1)\n",
    "        # 84 fully connected neurons\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # 10 fully connected neurons\n",
    "        self.fc3 = nn.Linear(84, output_dim,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flat(x)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "img_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Lambda(lambda x: torch.flatten(x))]\n",
    ")\n",
    "\n",
    "train_dataset = MNIST(\n",
    "    root=\"./data/MNIST\", download=True, train=True, transform=img_transform\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = MNIST(\n",
    "    root=\"./data/MNIST\", download=True, train=False, transform=img_transform\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=max(1000, batch_size), shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FullyConnectedNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ant/All/Sapienza/Deep Learning & AAI/Intrinsic-Dimension/intrinsic.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m FullyConnectedNetwork(\u001b[39m28\u001b[39m \u001b[39m*\u001b[39m \u001b[39m28\u001b[39m, \u001b[39m200\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m num_params \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters() \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mrequires_grad)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FullyConnectedNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "model = FullyConnectedNetwork(28 * 28, 200, 10)\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: %d\" % num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 44426\n"
     ]
    }
   ],
   "source": [
    "model = LeNet(1, 10)\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: %d\" % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 286334\n"
     ]
    }
   ],
   "source": [
    "model = Untied_LeNet(1, 10)\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: %d\" % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3640574\n"
     ]
    }
   ],
   "source": [
    "model = FcLeNet(28 * 28, 10)\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: %d\" % num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 193370\n"
     ]
    }
   ],
   "source": [
    "model = FCTied_LeNet(1, 10)\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: %d\" % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 300\n"
     ]
    }
   ],
   "source": [
    "model_intrinsic = DenseWrap(model, intrinsic_dimension=300, device=device)\n",
    "num_params = sum(p.numel() for p in model_intrinsic.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: %d\" % num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.292332\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.295081\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.294032\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.292007\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.293732\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.286234\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ant/All/Sapienza/Deep Learning & AAI/Intrinsic-Dimension/intrinsic.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X23sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X23sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X23sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     train(model_intrinsic, train_dataloader, optimizer, criterion, epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X23sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     accuracy \u001b[39m=\u001b[39m test(model_intrinsic, test_dataloader, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X23sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(accuracy))\n",
      "\u001b[1;32m/home/ant/All/Sapienza/Deep Learning & AAI/Intrinsic-Dimension/intrinsic.ipynb Cell 20\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ant/All/Sapienza/Deep%20Learning%20%26%20AAI/Intrinsic-Dimension/intrinsic.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)\n",
    "# train the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_intrinsic.parameters(), lr=0.1)\n",
    "\n",
    "# training step\n",
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "# testing step\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[\n",
    "                1\n",
    "            ]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "# train the model\n",
    "for epoch in range(1, 100):\n",
    "    train(model_intrinsic, train_dataloader, optimizer, criterion, epoch)\n",
    "    accuracy = test(model_intrinsic, test_dataloader, criterion)\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
