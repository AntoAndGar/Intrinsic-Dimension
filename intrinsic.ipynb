{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")\n",
    "\n",
    "# add reproducibility stuff\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastfoodWrapper(nn.Module):\n",
    "    def __init__(self, module, intrinsic_dimension, device):\n",
    "        \"\"\"\n",
    "        Wrapper to estimate the intrinsic dimensionality of the\n",
    "        objective landscape for a specific task given a specific model using FastFood transform\n",
    "        :param module: pytorch nn.Module\n",
    "        :param intrinsic_dimension: dimensionality within which we search for solution\n",
    "        :param device: cuda device id\n",
    "        \"\"\"\n",
    "        super(FastfoodWrapper, self).__init__()\n",
    "\n",
    "        # Hide this from inspection by get_parameters()\n",
    "        self.m = [module]\n",
    "\n",
    "        self.name_base_localname = []\n",
    "\n",
    "        # Stores the initial value: \\theta_{0}^{D}\n",
    "        self.initial_value = dict()\n",
    "\n",
    "        # Fastfood parameters\n",
    "        self.fastfood_params = {}\n",
    "\n",
    "        # Parameter vector that is updated\n",
    "        # Initialised with zeros as per text: \\theta^{d}\n",
    "        V = nn.Parameter(torch.zeros((intrinsic_dimension)).to(device))\n",
    "        self.register_parameter(\"V\", V)\n",
    "        v_size = (intrinsic_dimension,)\n",
    "\n",
    "        # Iterate over layers in the module\n",
    "        for name, param in module.named_parameters():\n",
    "            # If param requires grad update\n",
    "            if param.requires_grad:\n",
    "\n",
    "                # Saves the initial values of the initialised parameters from param.data and sets them to no grad.\n",
    "                # (initial values are the 'origin' of the search)\n",
    "                self.initial_value[name] = v0 = (\n",
    "                    param.clone().detach().requires_grad_(False).to(device)\n",
    "                )\n",
    "\n",
    "                # Generate fastfood parameters\n",
    "                DD = np.prod(v0.size())\n",
    "                self.fastfood_params[name] = fastfood_vars(DD, device)\n",
    "\n",
    "                base, localname = module, name\n",
    "                while \".\" in localname:\n",
    "                    prefix, localname = localname.split(\".\", 1)\n",
    "                    base = base.__getattr__(prefix)\n",
    "                self.name_base_localname.append((name, base, localname))\n",
    "\n",
    "        for name, base, localname in self.name_base_localname:\n",
    "            delattr(base, localname)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Iterate over layers\n",
    "        for name, base, localname in self.name_base_localname:\n",
    "\n",
    "            init_shape = self.initial_value[name].size()\n",
    "            DD = np.prod(init_shape)\n",
    "\n",
    "            # Fastfood transform te replace dence P\n",
    "            ray = fastfood_torched(self.V, DD, self.fastfood_params[name]).view(\n",
    "                init_shape\n",
    "            )\n",
    "\n",
    "            param = self.initial_value[name] + ray\n",
    "\n",
    "            setattr(base, localname, param)\n",
    "\n",
    "        # Pass through the model, by getting hte module from a list self.m\n",
    "        module = self.m[0]\n",
    "        x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fast_walsh_hadamard_torched(x, axis=0, normalize=False):\n",
    "    \"\"\"\n",
    "    Performs fast Walsh Hadamard transform\n",
    "    :param x:\n",
    "    :param axis:\n",
    "    :param normalize:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    orig_shape = x.size()\n",
    "    assert axis >= 0 and axis < len(orig_shape), (\n",
    "        \"For a vector of shape %s, axis must be in [0, %d] but it is %d\"\n",
    "        % (orig_shape, len(orig_shape) - 1, axis)\n",
    "    )\n",
    "    h_dim = orig_shape[axis]\n",
    "    h_dim_exp = int(round(np.log(h_dim) / np.log(2)))\n",
    "    assert h_dim == 2 ** h_dim_exp, (\n",
    "        \"hadamard can only be computed over axis with size that is a power of two, but\"\n",
    "        \" chosen axis %d has size %d\" % (axis, h_dim)\n",
    "    )\n",
    "\n",
    "    working_shape_pre = [int(np.prod(orig_shape[:axis]))]  # prod of empty array is 1 :)\n",
    "    working_shape_post = [\n",
    "        int(np.prod(orig_shape[axis + 1 :]))\n",
    "    ]  # prod of empty array is 1 :)\n",
    "    working_shape_mid = [2] * h_dim_exp\n",
    "    working_shape = working_shape_pre + working_shape_mid + working_shape_post\n",
    "\n",
    "    ret = x.view(working_shape)\n",
    "\n",
    "    for ii in range(h_dim_exp):\n",
    "        dim = ii + 1\n",
    "        arrs = torch.chunk(ret, 2, dim=dim)\n",
    "        assert len(arrs) == 2\n",
    "        ret = torch.cat((arrs[0] + arrs[1], arrs[0] - arrs[1]), axis=dim)\n",
    "\n",
    "    if normalize:\n",
    "        ret = ret / torch.sqrt(float(h_dim))\n",
    "\n",
    "    ret = ret.view(orig_shape)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fastfood_vars(DD, device=0):\n",
    "    \"\"\"\n",
    "    Returns parameters for fast food transform\n",
    "    :param DD: desired dimension\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ll = int(np.ceil(np.log(DD) / np.log(2)))\n",
    "    LL = 2 ** ll\n",
    "\n",
    "    # Binary scaling matrix where $B_{i,i} \\in \\{\\pm 1 \\}$ drawn iid\n",
    "    BB = torch.FloatTensor(LL).uniform_(0, 2).type(torch.LongTensor)\n",
    "    BB = (BB * 2 - 1).type(torch.FloatTensor).to(device)\n",
    "    BB.requires_grad = False\n",
    "\n",
    "    # Random permutation matrix\n",
    "    Pi = torch.LongTensor(np.random.permutation(LL)).to(device)\n",
    "    Pi.requires_grad = False\n",
    "\n",
    "    # Gaussian scaling matrix, whose elements $G_{i,i} \\sim \\mathcal{N}(0, 1)$\n",
    "    GG = torch.FloatTensor(LL,).normal_().to(device)\n",
    "    GG.requires_grad = False\n",
    "\n",
    "    divisor = torch.sqrt(LL * torch.sum(torch.pow(GG, 2)))\n",
    "\n",
    "    return [BB, Pi, GG, divisor, LL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fastfood_torched(x, DD, param_list=None, device=0):\n",
    "    \"\"\"\n",
    "    Fastfood transform\n",
    "    :param x: array of dd dimension\n",
    "    :param DD: desired dimension\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dd = x.size(0)\n",
    "\n",
    "    if not param_list:\n",
    "\n",
    "        BB, Pi, GG, divisor, LL = fastfood_vars(DD, device=device)\n",
    "\n",
    "    else:\n",
    "\n",
    "        BB, Pi, GG, divisor, LL = param_list\n",
    "\n",
    "    # Padd x if needed\n",
    "    dd_pad = F.pad(x, pad=(0, LL - dd), value=0, mode=\"constant\")\n",
    "\n",
    "    # From left to right HGPiH(BX), where H is Walsh-Hadamard matrix\n",
    "    mul_1 = torch.mul(BB, dd_pad)\n",
    "    # HGPi(HBX)\n",
    "    mul_2 = fast_walsh_hadamard_torched(mul_1, 0, normalize=False)\n",
    "\n",
    "    # HG(PiHBX)\n",
    "    mul_3 = mul_2[Pi]\n",
    "\n",
    "    # H(GPiHBX)\n",
    "    mul_4 = torch.mul(mul_3, GG)\n",
    "\n",
    "    # (HGPiHBX)\n",
    "    mul_5 = fast_walsh_hadamard_torched(mul_4, 0, normalize=False)\n",
    "\n",
    "    ret = torch.div(mul_5[:DD], divisor * np.sqrt(float(DD) / LL))\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DenseWrap(nn.Module):\n",
    "    def __init__(self, module, intrinsic_dimension, device):\n",
    "        \"\"\"\n",
    "        Wrapper to estimate the intrinsic dimensionality of the\n",
    "        objective landscape for a specific task given a specific model\n",
    "        :param module: pytorch nn.Module\n",
    "        :param intrinsic_dimension: dimensionality within which we search for solution\n",
    "        :param device: cuda device id\n",
    "        \"\"\"\n",
    "        super(DenseWrap, self).__init__()\n",
    "\n",
    "        # Hide this from inspection by get_parameters()\n",
    "        self.m = [module]\n",
    "\n",
    "        self.name_base_localname = []\n",
    "\n",
    "        # Stores the initial value: \\theta_{0}^{D}\n",
    "        self.initial_value = dict()\n",
    "\n",
    "        # Stores the randomly generated projection matrix P\n",
    "        self.random_matrix = dict()\n",
    "\n",
    "        # Parameter vector that is updated, initialised with zeros as per text: \\theta^{d}\n",
    "        V = nn.Parameter(torch.zeros((intrinsic_dimension, 1)).to(device))\n",
    "        self.register_parameter(\"V\", V)\n",
    "        v_size = (intrinsic_dimension,)\n",
    "\n",
    "        # Iterates over layers in the Neural Network\n",
    "        for name, param in module.named_parameters():\n",
    "            # If the parameter requires gradient update\n",
    "            if param.requires_grad:\n",
    "\n",
    "                # Saves the initial values of the initialised parameters from param.data and sets them to no grad.\n",
    "                # (initial values are the 'origin' of the search)\n",
    "                self.initial_value[name] = v0 = (\n",
    "                    param.clone().detach().requires_grad_(False).to(device)\n",
    "                )\n",
    "\n",
    "                # If v0.size() is [4, 3], then below operation makes it [4, 3, v_size]\n",
    "                matrix_size = v0.size() + v_size\n",
    "\n",
    "                # Generates random projection matrices P, sets them to no grad\n",
    "                self.random_matrix[name] = (\n",
    "                    torch.randn(matrix_size, requires_grad=False).to(device)\n",
    "                    / intrinsic_dimension ** 0.5\n",
    "                )\n",
    "\n",
    "                # NOTE!: lines below are not clear!\n",
    "                base, localname = module, name\n",
    "                while \".\" in localname:\n",
    "                    prefix, localname = localname.split(\".\", 1)\n",
    "                    base = base.__getattr__(prefix)\n",
    "                self.name_base_localname.append((name, base, localname))\n",
    "\n",
    "        for name, base, localname in self.name_base_localname:\n",
    "            delattr(base, localname)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Iterate over the layers\n",
    "        for name, base, localname in self.name_base_localname:\n",
    "\n",
    "            # Product between matrix P and \\theta^{d}\n",
    "            ray = torch.matmul(self.random_matrix[name], self.V)\n",
    "\n",
    "            # Add the \\theta_{0}^{D} to P \\dot \\theta^{d}\n",
    "            param = self.initial_value[name] + torch.squeeze(ray, -1)\n",
    "\n",
    "            setattr(base, localname, param)\n",
    "\n",
    "        # Pass through the model, by getting the module from a list self.m\n",
    "        module = self.m[0]\n",
    "        x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementig the code from the paper https://arxiv.org/abs/1804.08838 in pytorch\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, n_classes)\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def get_resnet(encoder_name, num_classes, pretrained=False):\n",
    "    assert encoder_name in [\"resnet18\", \"resnet50\"], \"{} is a wrong encoder name!\".format(encoder_name)\n",
    "    if encoder_name == \"resnet18\":\n",
    "        model = models.resnet18(pretrained=pretrained)\n",
    "        latent_dim = 512\n",
    "    else:\n",
    "        model = models.resnet50(pretrained=pretrained)\n",
    "        latent_dim = 2048\n",
    "    children = (list(model.children())[:-2] + [Classifier(latent_dim, num_classes)])\n",
    "    model = torch.nn.Sequential(*children)\n",
    "    return model\n",
    "\n",
    "# Get model and wrap it in fastfood\n",
    "model = get_resnet(\"resnet18\", num_classes=YOUR_NUMBER_OF_CLASSES).cuda()\n",
    "model = FastfoodWrapper(model, intrinsic_dimension=100, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for a Fully Connected Network\n",
    "class FullyConnectedNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: torch.flatten(x))\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=max(1000, batch_size), shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = FullyConnectedNetwork(28*28, 200, 10).to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# training step\n",
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "# testing step\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(1, 10):\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accuracy = test(model, test_dataloader, criterion)\n",
    "    print(\"Accuracy: {}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
